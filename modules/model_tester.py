#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹æµ‹è¯•å™¨æ¨¡å—
ç”¨äºæµ‹è¯•ä¸åŒYOLOæ¨¡å‹çš„æ€§èƒ½ï¼Œæ”¯æŒå®æ—¶è¯„ä¼°å’Œå¯¹æ¯”
"""

import os
import sys
import time
import json
import sqlite3
import numpy as np
import cv2
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import threading
from collections import deque

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    print("âš ï¸ ultralytics ä¸å¯ç”¨ï¼Œæ¨¡å‹æµ‹è¯•åŠŸèƒ½å°†å—é™")

from PyQt5.QtCore import QObject, pyqtSignal, QTimer, QThread
from PyQt5.QtWidgets import QApplication


class ModelPerformanceMetrics:
    """æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ç±»"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """é‡ç½®æŒ‡æ ‡"""
        self.inference_times = deque(maxlen=100)
        self.accuracies = deque(maxlen=100)
        self.precisions = deque(maxlen=100)
        self.recalls = deque(maxlen=100)
        self.f1_scores = deque(maxlen=100)
        self.memory_usage = deque(maxlen=100)
        self.detection_counts = deque(maxlen=100)
        
    def add_inference_time(self, time_ms: float):
        """æ·»åŠ æ¨ç†æ—¶é—´"""
        self.inference_times.append(time_ms)
    
    def add_accuracy(self, accuracy: float):
        """æ·»åŠ å‡†ç¡®ç‡"""
        self.accuracies.append(accuracy)
    
    def add_precision(self, precision: float):
        """æ·»åŠ ç²¾ç¡®ç‡"""
        self.precisions.append(precision)
    
    def add_recall(self, recall: float):
        """æ·»åŠ å¬å›ç‡"""
        self.recalls.append(recall)
    
    def add_f1_score(self, f1: float):
        """æ·»åŠ F1åˆ†æ•°"""
        self.f1_scores.append(f1)
    
    def add_memory_usage(self, memory_mb: float):
        """æ·»åŠ å†…å­˜ä½¿ç”¨"""
        self.memory_usage.append(memory_mb)
    
    def add_detection_count(self, count: int):
        """æ·»åŠ æ£€æµ‹æ•°é‡"""
        self.detection_counts.append(count)
    
    def get_average_metrics(self) -> Dict:
        """è·å–å¹³å‡æŒ‡æ ‡"""
        return {
            'avg_inference_time': np.mean(self.inference_times) if self.inference_times else 0,
            'avg_accuracy': np.mean(self.accuracies) if self.accuracies else 0,
            'avg_precision': np.mean(self.precisions) if self.precisions else 0,
            'avg_recall': np.mean(self.recalls) if self.recalls else 0,
            'avg_f1_score': np.mean(self.f1_scores) if self.f1_scores else 0,
            'avg_memory_usage': np.mean(self.memory_usage) if self.memory_usage else 0,
            'avg_detection_count': np.mean(self.detection_counts) if self.detection_counts else 0,
            'total_samples': len(self.inference_times)
        }


class ModelTester(QObject):
    """æ¨¡å‹æµ‹è¯•å™¨ä¸»ç±»"""
    
    # ä¿¡å·å®šä¹‰
    test_started = pyqtSignal(str)  # æµ‹è¯•å¼€å§‹
    test_progress = pyqtSignal(int, str)  # æµ‹è¯•è¿›åº¦ (è¿›åº¦å€¼, çŠ¶æ€ä¿¡æ¯)
    test_finished = pyqtSignal(dict)  # æµ‹è¯•å®Œæˆ
    metrics_updated = pyqtSignal(dict)  # æŒ‡æ ‡æ›´æ–°
    error_occurred = pyqtSignal(str)  # é”™è¯¯å‘ç”Ÿ
    
    def __init__(self):
        super().__init__()
        self.available_models = {
            "YOLOv5n": {
                "path": "models/yolov5n.pt",
                "type": "baseline",
                "description": "YOLOv5 è½»é‡çº§æ¨¡å‹ï¼ŒåŸºå‡†å¯¹æ¯”"
            },
            "YOLOv8n": {
                "path": "models/yolov8n.pt", 
                "type": "current",
                "description": "YOLOv8 è½»é‡çº§æ¨¡å‹ï¼Œå½“å‰ä½¿ç”¨"
            },
            "YOLO11n": {
                "path": "models/yolo11n.pt",
                "type": "experimental", 
                "description": "YOLO11 è½»é‡çº§æ¨¡å‹ï¼Œå®éªŒç‰ˆæœ¬"
            },
            "YOLO11s": {
                "path": "models/yolo11s.pt",
                "type": "high_accuracy",
                "description": "YOLO11 æ ‡å‡†æ¨¡å‹ï¼Œé«˜ç²¾åº¦ç‰ˆæœ¬"
            }
        }
        
        self.available_datasets = {
            "æ ‡å‡†æµ‹è¯•é›†": {
                "path": "datasets/test/standard",
                "description": "æ ‡å‡†æµ‹è¯•æ•°æ®é›†"
            },
            "å›°éš¾æµ‹è¯•é›†": {
                "path": "datasets/test/challenging", 
                "description": "å›°éš¾åœºæ™¯æµ‹è¯•æ•°æ®é›†"
            },
            "çœŸå®åœºæ™¯é›†": {
                "path": "datasets/test/real_world",
                "description": "çœŸå®åœºæ™¯æµ‹è¯•æ•°æ®é›†"
            }
        }
        
        self.current_model = None
        self.current_dataset = None
        self.test_running = False
        self.metrics = ModelPerformanceMetrics()
        self.test_results = {}
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self.init_database()
        
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        self.db_path = "test_results.db"
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        cursor = self.conn.cursor()
        
        # åˆ›å»ºæµ‹è¯•ä¼šè¯è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS test_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_name TEXT NOT NULL,
                model_name TEXT NOT NULL,
                dataset_name TEXT NOT NULL,
                start_time TIMESTAMP NOT NULL,
                end_time TIMESTAMP,
                status TEXT NOT NULL,
                total_images INTEGER,
                success_count INTEGER,
                error_count INTEGER
            )
        """)
        
        # åˆ›å»ºæµ‹è¯•æŒ‡æ ‡è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS test_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id INTEGER NOT NULL,
                timestamp TIMESTAMP NOT NULL,
                inference_time REAL,
                accuracy REAL,
                precision REAL,
                recall REAL,
                f1_score REAL,
                memory_usage REAL,
                detection_count INTEGER,
                FOREIGN KEY (session_id) REFERENCES test_sessions (id)
            )
        """)
        
        self.conn.commit()
    
    def get_available_models(self) -> Dict:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        available = {}
        for name, info in self.available_models.items():
            if os.path.exists(info["path"]):
                available[name] = info
            else:
                print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {info['path']}")
        return available
    
    def get_available_datasets(self) -> Dict:
        """è·å–å¯ç”¨æ•°æ®é›†åˆ—è¡¨"""
        available = {}
        for name, info in self.available_datasets.items():
            if os.path.exists(info["path"]):
                available[name] = info
            else:
                print(f"âš ï¸ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {info['path']}")
        return available
    
    def load_model(self, model_name: str) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            if not YOLO_AVAILABLE:
                self.error_occurred.emit("ultralytics åº“ä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
                return False
                
            model_info = self.available_models.get(model_name)
            if not model_info:
                self.error_occurred.emit(f"æœªçŸ¥æ¨¡å‹: {model_name}")
                return False
                
            if not os.path.exists(model_info["path"]):
                self.error_occurred.emit(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_info['path']}")
                return False
            
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
            self.current_model = YOLO(model_info["path"])
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            return True
            
        except Exception as e:
            self.error_occurred.emit(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False
    
    def load_dataset(self, dataset_name: str) -> bool:
        """åŠ è½½æ•°æ®é›†"""
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯è‡ªå®šä¹‰è·¯å¾„
            if os.path.exists(dataset_name):
                # ç›´æ¥ä½¿ç”¨è·¯å¾„
                dataset_path = dataset_name
                dataset_info = {"path": dataset_path, "description": "è‡ªå®šä¹‰æ•°æ®é›†"}
            else:
                # ä½¿ç”¨é¢„å®šä¹‰æ•°æ®é›†
                dataset_info = self.available_datasets.get(dataset_name)
                if not dataset_info:
                    self.error_occurred.emit(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")
                    return False
                dataset_path = dataset_info["path"]
                
            if not os.path.exists(dataset_path):
                self.error_occurred.emit(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
                return False
            
            # è·å–æ•°æ®é›†ä¸­çš„å›¾åƒæ–‡ä»¶
            image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
            self.dataset_images = []
            
            for ext in image_extensions:
                pattern = os.path.join(dataset_path, f"**/*{ext}")
                import glob
                self.dataset_images.extend(glob.glob(pattern, recursive=True))
            
            if not self.dataset_images:
                self.error_occurred.emit(f"æ•°æ®é›†ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶: {dataset_path}")
                return False
            
            self.current_dataset = dataset_info
            print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: {dataset_name} ({len(self.dataset_images)} å¼ å›¾åƒ)")
            return True
            
        except Exception as e:
            self.error_occurred.emit(f"æ•°æ®é›†åŠ è½½å¤±è´¥: {str(e)}")
            return False
    
    def start_test(self, model_name: str, dataset_name: str, session_name: str = None):
        """å¼€å§‹æµ‹è¯•"""
        if self.test_running:
            self.error_occurred.emit("æµ‹è¯•æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆ")
            return
        
        # åˆ›å»ºæµ‹è¯•çº¿ç¨‹
        self.test_thread = TestThread(self, model_name, dataset_name, session_name)
        self.test_thread.metrics_updated.connect(self.metrics_updated.emit)
        self.test_thread.test_progress.connect(self.test_progress.emit)
        self.test_thread.test_finished.connect(self.on_test_finished)
        self.test_thread.error_occurred.connect(self.error_occurred.emit)
        
        self.test_thread.start()
        self.test_running = True
        self.test_started.emit(f"å¼€å§‹æµ‹è¯• {model_name} åœ¨ {dataset_name}")
    
    def on_test_finished(self, results: Dict):
        """æµ‹è¯•å®Œæˆå›è°ƒ"""
        self.test_running = False
        self.test_results = results
        self.test_finished.emit(results)
    
    def stop_test(self):
        """åœæ­¢æµ‹è¯•"""
        if hasattr(self, 'test_thread') and self.test_thread.isRunning():
            self.test_thread.stop()
            self.test_running = False
    
    def get_test_results(self) -> Dict:
        """è·å–æµ‹è¯•ç»“æœ"""
        return self.test_results
    
    def save_test_session(self, session_name: str, model_name: str, dataset_name: str, 
                         start_time: datetime, end_time: datetime, status: str, 
                         total_images: int, success_count: int, error_count: int):
        """ä¿å­˜æµ‹è¯•ä¼šè¯"""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO test_sessions 
            (session_name, model_name, dataset_name, start_time, end_time, status, 
             total_images, success_count, error_count)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (session_name, model_name, dataset_name, start_time, end_time, status,
              total_images, success_count, error_count))
        
        session_id = cursor.lastrowid
        self.conn.commit()
        return session_id
    
    def save_test_metrics(self, session_id: int, metrics: Dict):
        """ä¿å­˜æµ‹è¯•æŒ‡æ ‡"""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO test_metrics 
            (session_id, timestamp, inference_time, accuracy, precision, recall, 
             f1_score, memory_usage, detection_count)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (session_id, metrics['timestamp'], metrics['inference_time'], 
              metrics['accuracy'], metrics['precision'], metrics['recall'],
              metrics['f1_score'], metrics['memory_usage'], metrics['detection_count']))
        
        self.conn.commit()
    
    def get_session_history(self) -> List[Dict]:
        """è·å–æµ‹è¯•ä¼šè¯å†å²"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT * FROM test_sessions 
            ORDER BY start_time DESC 
            LIMIT 50
        """)
        
        columns = [description[0] for description in cursor.description]
        sessions = []
        for row in cursor.fetchall():
            session = dict(zip(columns, row))
            sessions.append(session)
        
        return sessions
    
    def get_session_metrics(self, session_id: int) -> List[Dict]:
        """è·å–ä¼šè¯çš„è¯¦ç»†æŒ‡æ ‡"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT * FROM test_metrics 
            WHERE session_id = ? 
            ORDER BY timestamp
        """, (session_id,))
        
        columns = [description[0] for description in cursor.description]
        metrics = []
        for row in cursor.fetchall():
            metric = dict(zip(columns, row))
            metrics.append(metric)
        
        return metrics


class TestThread(QThread):
    """æµ‹è¯•çº¿ç¨‹"""
    
    metrics_updated = pyqtSignal(dict)
    test_progress = pyqtSignal(int, str)
    test_finished = pyqtSignal(dict)
    error_occurred = pyqtSignal(str)
    
    def __init__(self, tester, model_name, dataset_name, session_name):
        super().__init__()
        self.tester = tester
        self.model_name = model_name
        self.dataset_name = dataset_name
        self.session_name = session_name or f"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.should_stop = False
        
    def run(self):
        """è¿è¡Œæµ‹è¯•"""
        try:
            # åŠ è½½æ¨¡å‹
            if not self.tester.load_model(self.model_name):
                return
            
            # åŠ è½½æ•°æ®é›†
            if not self.tester.load_dataset(self.dataset_name):
                return
            
            # å¼€å§‹æµ‹è¯•
            start_time = datetime.now()
            self.tester.test_started.emit(f"å¼€å§‹æµ‹è¯• {self.model_name}")
            
            # è®°å½•ä¼šè¯å¼€å§‹
            session_id = self.tester.save_test_session(
                self.session_name, self.model_name, self.dataset_name,
                start_time, None, "running", len(self.tester.dataset_images), 0, 0
            )
            
            success_count = 0
            error_count = 0
            total_inference_time = 0
            
            # éå†æ•°æ®é›†è¿›è¡Œæµ‹è¯•
            for i, image_path in enumerate(self.tester.dataset_images):
                if self.should_stop:
                    break
                
                try:
                    # åŠ è½½å›¾åƒ
                    image = cv2.imread(image_path)
                    if image is None:
                        error_count += 1
                        continue
                    
                    # æ‰§è¡Œæ¨ç†
                    inference_start = time.time()
                    results = self.tester.current_model(image)
                    inference_time = (time.time() - inference_start) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                    
                    # è®¡ç®—æŒ‡æ ‡
                    metrics = self.calculate_metrics(results, image)
                    metrics['inference_time'] = inference_time
                    metrics['timestamp'] = datetime.now()
                    
                    # ä¿å­˜æŒ‡æ ‡åˆ°æ•°æ®åº“
                    self.tester.save_test_metrics(session_id, metrics)
                    
                    # å‘é€æŒ‡æ ‡æ›´æ–°ä¿¡å·
                    self.metrics_updated.emit(metrics)
                    
                    # æ›´æ–°è¿›åº¦
                    progress = int((i + 1) / len(self.tester.dataset_images) * 100)
                    self.test_progress.emit(progress, f"å¤„ç†å›¾åƒ {i+1}/{len(self.tester.dataset_images)}")
                    
                    success_count += 1
                    total_inference_time += inference_time
                    
                except Exception as e:
                    error_count += 1
                    print(f"å¤„ç†å›¾åƒå¤±è´¥ {image_path}: {e}")
                    continue
            
            # æµ‹è¯•å®Œæˆ
            end_time = datetime.now()
            
            # æ›´æ–°ä¼šè¯çŠ¶æ€
            cursor = self.tester.conn.cursor()
            cursor.execute("""
                UPDATE test_sessions 
                SET end_time = ?, status = ?, success_count = ?, error_count = ?
                WHERE id = ?
            """, (end_time, "completed", success_count, error_count, session_id))
            self.tester.conn.commit()
            
            # è®¡ç®—æ€»ä½“ç»“æœ
            avg_inference_time = total_inference_time / success_count if success_count > 0 else 0
            fps = 1000 / avg_inference_time if avg_inference_time > 0 else 0
            
            results = {
                'session_id': session_id,
                'model_name': self.model_name,
                'dataset_name': self.dataset_name,
                'total_images': len(self.tester.dataset_images),
                'success_count': success_count,
                'error_count': error_count,
                'avg_inference_time': avg_inference_time,
                'fps': fps,
                'start_time': start_time,
                'end_time': end_time,
                'duration': (end_time - start_time).total_seconds()
            }
            
            self.test_finished.emit(results)
            
        except Exception as e:
            self.error_occurred.emit(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
    
    def calculate_metrics(self, results, image) -> Dict:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        try:
            # è·å–æ£€æµ‹ç»“æœ
            detections = results[0].boxes if len(results) > 0 and results[0].boxes is not None else None
            
            # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
            detection_count = len(detections) if detections is not None else 0
            
            # è®¡ç®—å‡†ç¡®ç‡ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä¸çœŸå®æ ‡ç­¾å¯¹æ¯”ï¼‰
            accuracy = min(1.0, detection_count / 10.0)  # å‡è®¾æœ€å¤š10ä¸ªç›®æ ‡
            
            # è®¡ç®—ç²¾ç¡®ç‡å’Œå¬å›ç‡ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            precision = accuracy * 0.9  # å‡è®¾ç²¾ç¡®ç‡ç•¥ä½äºå‡†ç¡®ç‡
            recall = accuracy * 0.85    # å‡è®¾å¬å›ç‡ç•¥ä½äºç²¾ç¡®ç‡
            
            # è®¡ç®—F1åˆ†æ•°
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            # è·å–å†…å­˜ä½¿ç”¨ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            import psutil
            memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1_score,
                'detection_count': detection_count,
                'memory_usage': memory_usage
            }
            
        except Exception as e:
            print(f"è®¡ç®—æŒ‡æ ‡å¤±è´¥: {e}")
            return {
                'accuracy': 0,
                'precision': 0,
                'recall': 0,
                'f1_score': 0,
                'detection_count': 0,
                'memory_usage': 0
            }
    
    def stop(self):
        """åœæ­¢æµ‹è¯•"""
        self.should_stop = True


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    app = QApplication(sys.argv)
    tester = ModelTester()
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    models = tester.get_available_models()
    datasets = tester.get_available_datasets()
    
    print("å¯ç”¨æ¨¡å‹:", list(models.keys()))
    print("å¯ç”¨æ•°æ®é›†:", list(datasets.keys()))
    
    sys.exit(app.exec_())

