# -*- coding: utf-8 -*-
"""
ç›²é“éšœç¢æ£€æµ‹ç§»åŠ¨ç«¯åº”ç”¨
ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸“é—¨ç”¨äºAndroidéƒ¨ç½²
"""

import sys
import os
import cv2
import numpy as np
import json
import threading
import time
from PyQt5.QtWidgets import *
from PyQt5.QtCore import *
from PyQt5.QtGui import *
from PyQt5.QtMultimedia import QMediaPlayer, QMediaContent
from PyQt5.QtCore import QUrl
import requests
import base64
import tempfile
import uuid

# å¯¼å…¥è¯­éŸ³åº“
try:
    from voice_library import VoiceLibrary
    VOICE_LIBRARY_AVAILABLE = True
    print("âœ… è¯­éŸ³åº“å¯¼å…¥æˆåŠŸ")
except ImportError:
    VOICE_LIBRARY_AVAILABLE = False
    print("âš ï¸ è¯­éŸ³åº“å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤è¯­éŸ³æç¤º")

# å¯¼å…¥ç®€åŒ–çš„è¯­éŸ³ç³»ç»Ÿ
try:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from simple_voice_system import voice_system
    SIMPLE_VOICE_AVAILABLE = True
    print("âœ… ç®€åŒ–è¯­éŸ³ç³»ç»Ÿå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    SIMPLE_VOICE_AVAILABLE = False
    print(f"âš ï¸ ç®€åŒ–è¯­éŸ³ç³»ç»Ÿå¯¼å…¥å¤±è´¥: {e}")

# å¯¼å…¥è½¨è¿¹é¢„æµ‹ç³»ç»Ÿ
try:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from trajectory_predictor import TrajectoryPredictor, TrajectoryVisualizer
    TRAJECTORY_PREDICTION_AVAILABLE = True
    print("âœ… è½¨è¿¹é¢„æµ‹ç³»ç»Ÿå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    TRAJECTORY_PREDICTION_AVAILABLE = False
    print(f"âš ï¸ è½¨è¿¹é¢„æµ‹ç³»ç»Ÿå¯¼å…¥å¤±è´¥: {e}")

# å¯¼å…¥YOLO
try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
    print("âœ… YOLOå¯¼å…¥æˆåŠŸ")
except ImportError:
    YOLO_AVAILABLE = False
    print("âŒ YOLOå¯¼å…¥å¤±è´¥")

class BaiduTTS:
    """ç™¾åº¦è¯­éŸ³åˆæˆ"""
    
    def __init__(self, app_id, api_key, secret_key):
        self.app_id = app_id
        self.api_key = api_key
        self.secret_key = secret_key
        self.access_token = None
        self.token_expire_time = 0
        
    def get_access_token(self):
        """è·å–è®¿é—®ä»¤ç‰Œ"""
        if self.access_token and time.time() < self.token_expire_time:
            return self.access_token
            
        url = "https://aip.baidubce.com/oauth/2.0/token"
        params = {
            "grant_type": "client_credentials",
            "client_id": self.api_key,
            "client_secret": self.secret_key
        }
        
        try:
            response = requests.post(url, params=params)
            result = response.json()
            
            if "access_token" in result:
                self.access_token = result["access_token"]
                self.token_expire_time = time.time() + result["expires_in"] - 60
                print("âœ… ç™¾åº¦è¯­éŸ³ä»¤ç‰Œè·å–æˆåŠŸ")
                return self.access_token
            else:
                print(f"âŒ è·å–ç™¾åº¦è¯­éŸ³ä»¤ç‰Œå¤±è´¥: {result}")
                return None
        except Exception as e:
            print(f"âŒ è·å–ç™¾åº¦è¯­éŸ³ä»¤ç‰Œå¼‚å¸¸: {e}")
            return None
    
    def text_to_speech(self, text, output_file=None):
        """æ–‡æœ¬è½¬è¯­éŸ³"""
        token = self.get_access_token()
        if not token:
            return False
            
        url = "https://tsn.baidu.com/text2audio"
        
        data = {
            "tex": text,
            "tok": token,
            "cuid": "blind_road_detector",
            "ctp": "1",
            "lan": "zh",
            "spd": "5",
            "pit": "5",
            "vol": "5",
            "per": "0"
        }
        
        try:
            response = requests.post(url, data=data)
            
            if response.status_code == 200 and response.headers.get("Content-Type", "").startswith("audio"):
                if output_file is None:
                    output_file = os.path.join(tempfile.gettempdir(), f"blind_road_audio_{uuid.uuid4().hex[:8]}.mp3")
                
                with open(output_file, "wb") as f:
                    f.write(response.content)
                
                print(f"âœ… è¯­éŸ³åˆæˆæˆåŠŸ: {output_file}")
                return output_file
            else:
                print(f"âŒ è¯­éŸ³åˆæˆå¤±è´¥: {response.text}")
                return False
        except Exception as e:
            print(f"âŒ è¯­éŸ³åˆæˆå¼‚å¸¸: {e}")
            return False

class SimpleVoiceSystem:
    """ç®€åŒ–è¯­éŸ³ç³»ç»Ÿ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒè¿ç»­å®Œæ•´æ’­æŠ¥"""
    
    def __init__(self):
        self.media_player = None
        self.last_speak_text = ""
        self.speak_cooldown = 1.0  # å‡å°‘å†·å´æ—¶é—´
        self.last_speak_time = 0
        self.init_media_player()
        
        # ç™¾åº¦è¯­éŸ³é…ç½®
        self.baidu_client = None
        self.init_baidu_tts()
        
        # è¯­éŸ³é˜Ÿåˆ—ç®¡ç†
        self.voice_queue = []
        self.is_playing = False
        self.voice_lock = threading.Lock()
        
        # å¯åŠ¨è¯­éŸ³å¤„ç†çº¿ç¨‹
        self.start_voice_processor()
    
    def init_media_player(self):
        """åˆå§‹åŒ–åª’ä½“æ’­æ”¾å™¨"""
        try:
            self.media_player = QMediaPlayer()
            print("âœ… åª’ä½“æ’­æ”¾å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ åª’ä½“æ’­æ”¾å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.media_player = None
    
    def init_baidu_tts(self):
        """åˆå§‹åŒ–ç™¾åº¦è¯­éŸ³"""
        try:
            # ç™¾åº¦è¯­éŸ³APIé…ç½®
            app_id = "119634292"
            api_key = "w978fA2S7PJmUy4IEvlGqxfx"
            secret_key = "ZeTBNN1UYQRL1kaDEEImHm07Y09jgaRc"
            
            self.baidu_client = BaiduTTS(app_id, api_key, secret_key)
            print("âœ… ç™¾åº¦è¯­éŸ³åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ ç™¾åº¦è¯­éŸ³åˆå§‹åŒ–å¤±è´¥: {e}")
            self.baidu_client = None
    
    def start_voice_processor(self):
        """å¯åŠ¨è¯­éŸ³å¤„ç†çº¿ç¨‹"""
        def voice_processor():
            while True:
                try:
                    with self.voice_lock:
                        if self.voice_queue and not self.is_playing:
                            text = self.voice_queue.pop(0)
                            self.is_playing = True
                        else:
                            text = None
                    
                    if text:
                        self.execute_speech(text)
                        with self.voice_lock:
                            self.is_playing = False
                    else:
                        time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…
                        
                except Exception as e:
                    print(f"âš ï¸ è¯­éŸ³å¤„ç†çº¿ç¨‹é”™è¯¯: {e}")
                    with self.voice_lock:
                        self.is_playing = False
        
        voice_thread = threading.Thread(target=voice_processor, daemon=True)
        voice_thread.start()
        print("âœ… è¯­éŸ³å¤„ç†çº¿ç¨‹å·²å¯åŠ¨")
    
    def speak(self, text, priority=1):
        """è¯­éŸ³æ’­æŠ¥ - æ”¯æŒä¼˜å…ˆçº§å’Œé˜Ÿåˆ—ç®¡ç†"""
        if not text:
            return
        
        # æ£€æŸ¥å†·å´æ—¶é—´ï¼ˆæ ¹æ®ä¼˜å…ˆçº§è°ƒæ•´ï¼‰
        current_time = time.time()
        cooldown = 0.5 if priority >= 3 else 1.0
        if current_time - self.last_speak_time < cooldown:
            return
        
        # æ£€æŸ¥æ˜¯å¦é‡å¤
        if text == self.last_speak_text:
            return
        
        self.last_speak_text = text
        self.last_speak_time = current_time
        
        print(f"ğŸ”Š è¯­éŸ³æ’­æŠ¥ (ä¼˜å…ˆçº§{priority}): {text}")
        
        # æ·»åŠ åˆ°è¯­éŸ³é˜Ÿåˆ—
        with self.voice_lock:
            if priority >= 3:
                # é«˜ä¼˜å…ˆçº§ï¼Œæ’å…¥åˆ°é˜Ÿåˆ—å‰é¢
                self.voice_queue.insert(0, text)
            else:
                # æ™®é€šä¼˜å…ˆçº§ï¼Œæ·»åŠ åˆ°é˜Ÿåˆ—æœ«å°¾
                self.voice_queue.append(text)
    
    def execute_speech(self, text):
        """æ‰§è¡Œè¯­éŸ³æ’­æŠ¥"""
        try:
            # ä½¿ç”¨ç™¾åº¦è¯­éŸ³åˆæˆ
            if self.baidu_client:
                audio_file = self.baidu_client.text_to_speech(text)
                if audio_file:
                    self.play_audio(audio_file)
                    return
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨pyttsx3
            try:
                import pyttsx3
                tts = pyttsx3.init()
                tts.setProperty('rate', 150)
                tts.say(text)
                tts.runAndWait()
                tts.stop()
                print(f"âœ… å¤‡ç”¨è¯­éŸ³æ’­æŠ¥æˆåŠŸ: {text}")
            except Exception as e:
                print(f"âš ï¸ å¤‡ç”¨è¯­éŸ³æ’­æŠ¥å¤±è´¥: {e}")
                
        except Exception as e:
            print(f"âŒ è¯­éŸ³æ’­æŠ¥æ‰§è¡Œå¤±è´¥: {e}")
    
    def play_audio(self, audio_file):
        """æ’­æ”¾éŸ³é¢‘"""
        if not audio_file or not os.path.exists(audio_file):
            return
            
        try:
            if self.media_player:
                # GUIå†…åµŒæ’­æ”¾
                url = QUrl.fromLocalFile(audio_file)
                content = QMediaContent(url)
                self.media_player.setMedia(content)
                self.media_player.play()
                print(f"âœ… éŸ³é¢‘æ’­æ”¾æˆåŠŸ: {audio_file}")
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šç³»ç»Ÿæ’­æ”¾å™¨
                import subprocess
                subprocess.Popen(['start', audio_file], shell=True)
                print(f"âœ… ç³»ç»Ÿæ’­æ”¾å™¨æ’­æ”¾: {audio_file}")
        except Exception as e:
            print(f"âŒ éŸ³é¢‘æ’­æ”¾å¤±è´¥: {e}")

class MobileDetectWindow(QMainWindow):
    """ç§»åŠ¨ç«¯æ£€æµ‹çª—å£"""
    
    def __init__(self):
        super().__init__()
        self.cap = None
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_frame)
        
        # YOLOæ¨¡å‹
        self.model = None
        self.init_yolo_model()
        
        # è¯­éŸ³ç³»ç»Ÿ
        if SIMPLE_VOICE_AVAILABLE:
            self.voice_system = voice_system
            print("âœ… ä½¿ç”¨ç®€åŒ–è¯­éŸ³ç³»ç»Ÿ")
        else:
            self.voice_system = SimpleVoiceSystem()
            print("âš ï¸ ä½¿ç”¨å¤‡ç”¨è¯­éŸ³ç³»ç»Ÿ")
        
        # è¯­éŸ³åº“
        self.voice_library = None
        if VOICE_LIBRARY_AVAILABLE:
            self.voice_library = VoiceLibrary()
            print("âœ… è¯­éŸ³åº“åˆå§‹åŒ–æˆåŠŸ")
        
        # çŠ¶æ€è·Ÿè¸ª
        self.last_distance = None
        self.last_direction = None
        self.last_label = None
        
        # è½¨è¿¹é¢„æµ‹ç³»ç»Ÿ
        if TRAJECTORY_PREDICTION_AVAILABLE:
            self.trajectory_predictor = TrajectoryPredictor()
            self.trajectory_visualizer = TrajectoryVisualizer()
            print("âœ… è½¨è¿¹é¢„æµ‹ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        else:
            self.trajectory_predictor = None
            self.trajectory_visualizer = None
            print("âš ï¸ è½¨è¿¹é¢„æµ‹ç³»ç»Ÿä¸å¯ç”¨")
        
        self.init_ui()
    
    def init_yolo_model(self):
        """åˆå§‹åŒ–YOLOæ¨¡å‹"""
        try:
            model_path = "runs/detect/train5/weights/best.pt"
            if os.path.exists(model_path):
                self.model = YOLO(model_path)
                print(f"âœ… åŠ è½½è‡ªå®šä¹‰æ¨¡å‹: {model_path}")
            else:
                self.model = YOLO("yolov8n.pt")
                print("âœ… åŠ è½½é»˜è®¤æ¨¡å‹: yolov8n.pt")
        except Exception as e:
            print(f"âŒ YOLOæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model = None
    
    def init_ui(self):
        """åˆå§‹åŒ–UI"""
        self.setWindowTitle("ç›²é“éšœç¢æ£€æµ‹ - ç§»åŠ¨ç«¯")
        self.setGeometry(100, 100, 800, 600)
        
        # åˆ›å»ºä¸­å¤®éƒ¨ä»¶
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        
        # ä¸»å¸ƒå±€
        layout = QVBoxLayout(central_widget)
        
        # æ‘„åƒå¤´æ˜¾ç¤ºåŒºåŸŸ
        self.camera_label = QLabel()
        self.camera_label.setMinimumSize(640, 480)
        self.camera_label.setStyleSheet("border: 2px solid gray;")
        self.camera_label.setAlignment(Qt.AlignCenter)
        self.camera_label.setText("ç‚¹å‡»å¼€å§‹æ£€æµ‹")
        layout.addWidget(self.camera_label)
        
        # æ§åˆ¶æŒ‰é’®
        button_layout = QHBoxLayout()
        
        self.start_btn = QPushButton("å¼€å§‹æ£€æµ‹")
        self.start_btn.clicked.connect(self.toggle_camera)
        button_layout.addWidget(self.start_btn)
        
        self.voice_btn = QPushButton("è¯­éŸ³å¼€å…³")
        self.voice_btn.setCheckable(True)
        self.voice_btn.setChecked(True)
        button_layout.addWidget(self.voice_btn)
        
        self.test_btn = QPushButton("æµ‹è¯•è¯­éŸ³")
        self.test_btn.clicked.connect(self.test_voice)
        button_layout.addWidget(self.test_btn)
        
        layout.addLayout(button_layout)
        
        # çŠ¶æ€æ˜¾ç¤º
        self.status_label = QLabel("å‡†å¤‡å°±ç»ª")
        self.status_label.setStyleSheet("font-size: 14px; color: blue;")
        layout.addWidget(self.status_label)
        
        # æ£€æµ‹ä¿¡æ¯æ˜¾ç¤º
        self.info_label = QLabel("")
        self.info_label.setStyleSheet("font-size: 12px; color: green;")
        layout.addWidget(self.info_label)
    
    def toggle_camera(self):
        """åˆ‡æ¢æ‘„åƒå¤´çŠ¶æ€"""
        if self.cap is None or not self.cap.isOpened():
            self.start_camera()
        else:
            self.stop_camera()
    
    def start_camera(self):
        """å¯åŠ¨æ‘„åƒå¤´"""
        try:
            self.cap = cv2.VideoCapture(0)
            if self.cap.isOpened():
                self.timer.start(30)  # 30msé—´éš”
                self.start_btn.setText("åœæ­¢æ£€æµ‹")
                self.status_label.setText("æ£€æµ‹ä¸­...")
                print("âœ… æ‘„åƒå¤´å¯åŠ¨æˆåŠŸ")
            else:
                self.status_label.setText("æ‘„åƒå¤´å¯åŠ¨å¤±è´¥")
                print("âŒ æ‘„åƒå¤´å¯åŠ¨å¤±è´¥")
        except Exception as e:
            self.status_label.setText(f"æ‘„åƒå¤´é”™è¯¯: {e}")
            print(f"âŒ æ‘„åƒå¤´é”™è¯¯: {e}")
    
    def stop_camera(self):
        """åœæ­¢æ‘„åƒå¤´"""
        if self.timer.isActive():
            self.timer.stop()
        if self.cap:
            self.cap.release()
            self.cap = None
        self.start_btn.setText("å¼€å§‹æ£€æµ‹")
        self.status_label.setText("å·²åœæ­¢")
        self.camera_label.setText("ç‚¹å‡»å¼€å§‹æ£€æµ‹")
        print("âœ… æ‘„åƒå¤´å·²åœæ­¢")
    
    def update_frame(self):
        """æ›´æ–°æ‘„åƒå¤´å¸§"""
        if not self.cap or not self.cap.isOpened():
            return
            
        ret, frame = self.cap.read()
        if not ret:
            return
        
        # YOLOæ£€æµ‹
        if self.model:
            results = self.model(frame, verbose=False)
            current_time = time.time()
            
            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for i, box in enumerate(boxes):
                        # è·å–æ£€æµ‹ç»“æœ
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        conf = box.conf[0].cpu().numpy()
                        cls = int(box.cls[0].cpu().numpy())
                        
                        # æ˜¾ç¤ºæ‰€æœ‰æ£€æµ‹ç»“æœï¼Œä¸ç®¡ç½®ä¿¡åº¦
                        print(f"ğŸ” æ£€æµ‹åˆ°: {self.get_class_name(cls)} ç½®ä¿¡åº¦: {conf:.2f}")
                        
                        if conf > 0.1:  # æä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œç¡®ä¿æ˜¾ç¤ºæ‰€æœ‰æ£€æµ‹
                            # ç»˜åˆ¶è¾¹ç•Œæ¡†
                            color = self.get_class_color(cls)
                            thickness = max(4, int(6 * conf))  # è¿›ä¸€æ­¥å¢åŠ çº¿æ¡ç²—ç»†
                            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness)
                            
                            # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
                            label_text = f"{self.get_class_name(cls)} {conf:.2f}"
                            label_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]
                            
                            # æ ‡ç­¾èƒŒæ™¯
                            cv2.rectangle(frame, (int(x1), int(y1) - label_size[1] - 10), 
                                        (int(x1) + label_size[0] + 10, int(y1)), color, -1)
                            
                            # ç»˜åˆ¶æ ‡ç­¾æ–‡æœ¬
                            cv2.putText(frame, label_text, (int(x1) + 5, int(y1) - 5), 
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                            
                            # è®¡ç®—è·ç¦»å’Œæ–¹å‘
                            distance = self.estimate_distance(y2 - y1)
                            direction = self.get_direction(x1, x2, frame.shape[1])
                            
                            # è·å–æ ‡ç­¾åç§°
                            label_name = self.get_class_name(cls)
                            
                            # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
                            label_text = f"{label_name} {conf:.2f}"
                            distance_text = f"{distance:.1f}m"
                            direction_text = direction
                            
                            # è®¡ç®—æ–‡æœ¬å°ºå¯¸
                            font_scale = 0.6
                            font_thickness = 2
                            (label_w, label_h), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)
                            (dist_w, dist_h), _ = cv2.getTextSize(distance_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)
                            
                            # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
                            label_y = int(y1) - 10
                            if label_y < label_h:
                                label_y = int(y1) + label_h + 10
                            
                            # æ ‡ç­¾èƒŒæ™¯
                            cv2.rectangle(frame, (int(x1), label_y - label_h - 5), 
                                        (int(x1) + max(label_w, dist_w) + 10, label_y + 5), color, -1)
                            
                            # ç»˜åˆ¶æ–‡æœ¬
                            cv2.putText(frame, label_text, (int(x1) + 5, label_y - 5), 
                                      cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)
                            
                            # è·ç¦»æ–‡æœ¬
                            cv2.putText(frame, distance_text, (int(x1) + 5, label_y + dist_h), 
                                      cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)
                            
                            # ç»˜åˆ¶æ–¹å‘æŒ‡ç¤ºå™¨
                            center_x = int((x1 + x2) / 2)
                            center_y = int((y1 + y2) / 2)
                            self.draw_direction_indicator(frame, center_x, center_y, direction, color)
                            
                            # ç»˜åˆ¶ç½®ä¿¡åº¦æ¡
                            self.draw_confidence_bar(frame, int(x1), int(y1), int(x2), int(y2), conf, color)
                            
                            # è½¨è¿¹é¢„æµ‹
                            if self.trajectory_predictor:
                                object_id = f"obj_{i}_{cls}_{int(current_time*1000)}"
                                bbox = (int(x1), int(y1), int(x2), int(y2))
                                self.trajectory_predictor.update_trajectory(object_id, bbox, current_time)
                                
                                # è·å–è½¨è¿¹é¢„æµ‹
                                prediction = self.trajectory_predictor.predict_trajectory(object_id)
                                if prediction:
                                    # ç»˜åˆ¶è½¨è¿¹é¢„æµ‹
                                    frame = self.trajectory_visualizer.draw_trajectory(frame, prediction)
                                    
                                    # åŸºäºè½¨è¿¹é¢„æµ‹çš„è¯­éŸ³æ’­æŠ¥
                                    if prediction.collision_risk > 0.7:
                                        risk_msg = f"è­¦å‘Šï¼{label_name}è½¨è¿¹é¢„æµ‹æ˜¾ç¤ºé«˜é£é™©ï¼Œå»ºè®®åœæ­¢"
                                        if self.voice_btn.isChecked():
                                            self.voice_system.speak_urgent(risk_msg)
                                    elif prediction.collision_risk > 0.4:
                                        risk_msg = f"æ³¨æ„ï¼{label_name}è½¨è¿¹é¢„æµ‹æ˜¾ç¤ºä¸­é£é™©ï¼Œè¯·å‡é€Ÿ"
                                        if self.voice_btn.isChecked():
                                            self.voice_system.speak(risk_msg)
                                    
                                    # æ›´æ–°çŠ¶æ€æ˜¾ç¤º
                                    self.status_label.setText(f"è½¨è¿¹é¢„æµ‹: é£é™©{prediction.collision_risk:.2f}, TTC:{prediction.time_to_collision:.1f}s")
                                    
                                    # æ¸…ç†æ—§è½¨è¿¹
                                    self.trajectory_predictor.cleanup_old_trajectories(current_time, max_age=3.0)
                            
                            # è¯­éŸ³æ’­æŠ¥
                            if self.voice_btn.isChecked():
                                self.on_detected(cls, distance, label_name, direction)
        
        # è½¬æ¢ä¸ºQtæ ¼å¼å¹¶æ˜¾ç¤º
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb_frame.shape
        bytes_per_line = ch * w
        qt_image = QImage(rgb_frame.data, w, h, bytes_per_line, QImage.Format_RGB888)
        pixmap = QPixmap.fromImage(qt_image)
        
        # ç¼©æ”¾ä»¥é€‚åº”æ˜¾ç¤ºåŒºåŸŸ
        scaled_pixmap = pixmap.scaled(self.camera_label.size(), Qt.KeepAspectRatio)
        self.camera_label.setPixmap(scaled_pixmap)
    
    def get_class_color(self, cls):
        """è·å–ç±»åˆ«é¢œè‰²"""
        colors = [
            (0, 255, 0),    # ç»¿è‰² - é™æ€éšœç¢
            (255, 0, 0),    # çº¢è‰² - åŠ¨æ€éšœç¢
            (0, 0, 255),    # è“è‰² - åœ°é¢å¼‚å¸¸
            (255, 255, 0),  # é»„è‰² - ç¯å¢ƒå£°éŸ³
        ]
        return colors[cls % len(colors)]
    
    def get_class_name(self, cls):
        """è·å–ç±»åˆ«åç§°"""
        names = ["è½¦è¾†", "è¡Œäºº", "å‘æ´¼", "æ‘Šä½", "åƒåœ¾æ¡¶", "å® ç‰©", "å°é˜¶", "æ–œå¡", "ç§¯æ°´", "å®¶å…·", "ç§»åŠ¨è½¦è¾†"]
        return names[cls] if cls < len(names) else f"éšœç¢ç‰©{cls}"
    
    def estimate_distance(self, box_height):
        """ä¼°ç®—è·ç¦»ï¼ˆåŸºäºè¾¹ç•Œæ¡†é«˜åº¦ï¼‰"""
        # ç®€åŒ–çš„è·ç¦»ä¼°ç®—
        focal_length = 1000  # ç„¦è·
        real_height = 1.0    # å‡è®¾çœŸå®é«˜åº¦1ç±³
        distance = (focal_length * real_height) / box_height
        return max(0.1, min(10.0, distance))  # é™åˆ¶åœ¨0.1-10ç±³èŒƒå›´å†…
    
    def get_direction(self, x1, x2, frame_width):
        """è·å–æ–¹å‘"""
        center_x = (x1 + x2) / 2
        if center_x < frame_width * 0.4:
            return "å·¦ä¾§"
        elif center_x > frame_width * 0.6:
            return "å³ä¾§"
        else:
            return "æ­£å‰æ–¹"
    
    def draw_direction_indicator(self, frame, center_x, center_y, direction, color):
        """ç»˜åˆ¶æ–¹å‘æŒ‡ç¤ºå™¨"""
        arrow_size = 15
        if direction == "å·¦ä¾§":
            # å·¦ç®­å¤´
            points = np.array([
                [center_x - arrow_size, center_y],
                [center_x, center_y - arrow_size//2],
                [center_x, center_y + arrow_size//2]
            ], np.int32)
        elif direction == "å³ä¾§":
            # å³ç®­å¤´
            points = np.array([
                [center_x + arrow_size, center_y],
                [center_x, center_y - arrow_size//2],
                [center_x, center_y + arrow_size//2]
            ], np.int32)
        else:
            # ä¸Šç®­å¤´ï¼ˆæ­£å‰æ–¹ï¼‰
            points = np.array([
                [center_x, center_y - arrow_size],
                [center_x - arrow_size//2, center_y],
                [center_x + arrow_size//2, center_y]
            ], np.int32)
        
        cv2.fillPoly(frame, [points], color)
    
    def draw_confidence_bar(self, frame, x1, y1, x2, y2, confidence, color):
        """ç»˜åˆ¶ç½®ä¿¡åº¦æ¡"""
        bar_width = 4
        bar_height = int((y2 - y1) * confidence)
        bar_x = x2 + 5
        bar_y = y2 - bar_height
        
        # ç»˜åˆ¶èƒŒæ™¯
        cv2.rectangle(frame, (bar_x, y1), (bar_x + bar_width, y2), (100, 100, 100), -1)
        
        # ç»˜åˆ¶ç½®ä¿¡åº¦æ¡
        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, y2), color, -1)
        
        # ç»˜åˆ¶ç½®ä¿¡åº¦æ–‡æœ¬
        conf_text = f"{confidence:.2f}"
        cv2.putText(frame, conf_text, (bar_x + bar_width + 2, bar_y + 15), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
    
    def on_detected(self, cls, distance, label_name, direction):
        """æ£€æµ‹åˆ°éšœç¢ç‰©æ—¶çš„å¤„ç†"""
        try:
            print(f"ğŸ” æ£€æµ‹åˆ°éšœç¢ç‰©: {label_name}, è·ç¦»: {distance:.1f}ç±³, æ–¹ä½: {direction}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾è‘—å˜åŒ–
            distance_changed = (self.last_distance is None or 
                              abs(distance - self.last_distance) > 0.5)
            direction_changed = (self.last_direction != direction)
            label_changed = (self.last_label != label_name)
            
            should_speak = distance_changed or direction_changed or label_changed
            
            # ç›´æ¥ä½¿ç”¨ä¿®å¤çš„è¯­éŸ³ç³»ç»Ÿ
            if should_speak:
                if distance > 5:
                    self.status_label.setText("")
                    print("ğŸ“ è·ç¦»è¿‡è¿œï¼Œä¸æ’­æŠ¥")
                elif distance > 3:
                    msg = f"å‰æ–¹{distance:.1f}ç±³{direction}æœ‰{label_name}"
                    self.status_label.setText(msg)
                    self.info_label.setText(f"æ£€æµ‹: {label_name} | è·ç¦»: {distance:.1f}m | æ–¹å‘: {direction}")
                    print(f"ğŸ“¢ è¯­éŸ³æç¤º: {msg}")
                    self.voice_system.speak(msg)
                elif distance > 1:
                    msg = f"{direction}{distance:.1f}ç±³æœ‰{label_name}ï¼Œè¯·å‡é€Ÿ"
                    self.status_label.setText(msg)
                    self.info_label.setText(f"æ£€æµ‹: {label_name} | è·ç¦»: {distance:.1f}m | æ–¹å‘: {direction}")
                    print(f"ğŸ“¢ è¯­éŸ³æç¤º: {msg}")
                    self.voice_system.speak(msg)
                else:
                    msg = f"å±é™©ï¼ç«‹å³åœæ­¢ï¼{direction}{distance:.1f}ç±³æœ‰{label_name}"
                    self.status_label.setText(msg)
                    self.info_label.setText(f"æ£€æµ‹: {label_name} | è·ç¦»: {distance:.1f}m | æ–¹å‘: {direction}")
                    print(f"ğŸ“¢ è¯­éŸ³æç¤º: {msg}")
                    self.voice_system.speak_urgent(msg)
            
            # æ›´æ–°çŠ¶æ€
            self.last_distance = distance
            self.last_direction = direction
            self.last_label = label_name
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ£€æµ‹ç»“æœæ—¶å‡ºé”™: {e}")
    
    def test_voice(self):
        """æµ‹è¯•è¯­éŸ³"""
        test_msg = "è¯­éŸ³ç³»ç»Ÿæµ‹è¯•æˆåŠŸ"
        self.status_label.setText(test_msg)
        self.voice_system.speak(test_msg)
        print(f"ğŸ”Š è¯­éŸ³æµ‹è¯•: {test_msg}")
    
    def closeEvent(self, event):
        """å…³é—­äº‹ä»¶"""
        self.stop_camera()
        event.accept()

def main():
    app = QApplication(sys.argv)
    window = MobileDetectWindow()
    window.show()
    sys.exit(app.exec_())

if __name__ == "__main__":
    main() 