#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›²é“æ£€æµ‹æ•°æ®é›†ä¸‹è½½å™¨
è‡ªåŠ¨ä¸‹è½½å’Œæ•´ç†å…¬å¼€çš„ç›²é“æ£€æµ‹æ•°æ®é›†
"""

import os
import requests
import zipfile
import json
import shutil
from pathlib import Path
from typing import Dict, List, Optional
import cv2
import numpy as np
from tqdm import tqdm
import yaml

class BlindRoadDatasetDownloader:
    """ç›²é“æ£€æµ‹æ•°æ®é›†ä¸‹è½½å™¨"""
    
    def __init__(self, base_dir: str = "datasets"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        
        # æ•°æ®é›†é…ç½®
        self.datasets = {
            "blind_road_basic": {
                "name": "ç›²é“è¯†åˆ«åŸºç¡€æ•°æ®é›†",
                "description": "åŒ…å«381å¼ ç›²é“å›¾åƒï¼Œ233å¼ æœ‰ç›²é“ï¼Œ148å¼ æ— ç›²é“",
                "size": "50x50åƒç´ ",
                "format": "åˆ†ç±»",
                "url": "https://aistudio.baidu.com/aistudio/datasetdetail/123456",  # éœ€è¦æ›¿æ¢ä¸ºå®é™…URL
                "local_path": "blind_road_basic"
            },
            "unitree_blind_road": {
                "name": "Unitreeç›²é“åˆ†å‰²æ•°æ®é›†",
                "description": "500å¤šå¼ å®¤å¤–ç›²é“åˆ†å‰²å›¾åƒ",
                "size": "é«˜åˆ†è¾¨ç‡",
                "format": "åˆ†å‰²",
                "url": "https://aistudio.baidu.com/aistudio/datasetdetail/123457",  # éœ€è¦æ›¿æ¢ä¸ºå®é™…URL
                "local_path": "unitree_blind_road"
            },
            "blind_road_damage": {
                "name": "ç›²é“æŸåæ£€æµ‹æ•°æ®é›†",
                "description": "4426å¼ å›¾ç‰‡ï¼Œ3ä¸ªç±»åˆ«ï¼ŒåŒ…å«Pascal VOCå’ŒYOLOæ ¼å¼",
                "size": "å¤šåˆ†è¾¨ç‡",
                "format": "æ£€æµ‹",
                "url": "https://example.com/blind_road_damage.zip",  # éœ€è¦æ›¿æ¢ä¸ºå®é™…URL
                "local_path": "blind_road_damage"
            },
            "coco_pedestrian": {
                "name": "COCOè¡Œäººæ£€æµ‹æ•°æ®é›†",
                "description": "åŒ…å«è¡Œäººå’Œé“è·¯åœºæ™¯ï¼Œå¯ç”¨äºè¿ç§»å­¦ä¹ ",
                "size": "é«˜åˆ†è¾¨ç‡",
                "format": "æ£€æµ‹",
                "url": "http://images.cocodataset.org/zips/val2017.zip",
                "local_path": "coco_pedestrian"
            }
        }
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self.setup_directories()
    
    def setup_directories(self):
        """è®¾ç½®ç›®å½•ç»“æ„"""
        dirs = [
            "raw",           # åŸå§‹æ•°æ®
            "processed",     # å¤„ç†åæ•°æ®
            "yolo_format",   # YOLOæ ¼å¼
            "annotations",   # æ ‡æ³¨æ–‡ä»¶
            "images",        # å›¾åƒæ–‡ä»¶
            "train",         # è®­ç»ƒé›†
            "val",           # éªŒè¯é›†
            "test"           # æµ‹è¯•é›†
        ]
        
        for dir_name in dirs:
            (self.base_dir / dir_name).mkdir(exist_ok=True)
    
    def download_dataset(self, dataset_name: str, force_download: bool = False) -> bool:
        """ä¸‹è½½æŒ‡å®šæ•°æ®é›†"""
        if dataset_name not in self.datasets:
            print(f"âŒ æœªçŸ¥æ•°æ®é›†: {dataset_name}")
            return False
        
        dataset_info = self.datasets[dataset_name]
        local_path = self.base_dir / "raw" / dataset_info["local_path"]
        
        if local_path.exists() and not force_download:
            print(f"âœ… æ•°æ®é›†å·²å­˜åœ¨: {local_path}")
            return True
        
        print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ•°æ®é›†: {dataset_info['name']}")
        print(f"   æè¿°: {dataset_info['description']}")
        print(f"   æ ¼å¼: {dataset_info['format']}")
        
        try:
            # åˆ›å»ºç›®æ ‡ç›®å½•
            local_path.mkdir(parents=True, exist_ok=True)
            
            # æ¨¡æ‹Ÿä¸‹è½½è¿‡ç¨‹ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦æ›¿æ¢ä¸ºçœŸå®URLï¼‰
            if dataset_name == "coco_pedestrian":
                return self.download_coco_dataset(local_path)
            else:
                return self.create_synthetic_dataset(dataset_name, local_path)
                
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def download_coco_dataset(self, local_path: Path) -> bool:
        """ä¸‹è½½COCOæ•°æ®é›†"""
        try:
            print("ğŸ“¥ ä¸‹è½½COCOéªŒè¯é›†...")
            # è¿™é‡Œåº”è¯¥å®ç°çœŸå®çš„ä¸‹è½½é€»è¾‘
            # ç”±äºCOCOæ•°æ®é›†å¾ˆå¤§ï¼Œè¿™é‡Œåªæ˜¯ç¤ºä¾‹
            print("âš ï¸ æ³¨æ„: COCOæ•°æ®é›†è¾ƒå¤§ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½å¹¶æ”¾ç½®åˆ°ç›¸åº”ç›®å½•")
            return True
        except Exception as e:
            print(f"âŒ COCOæ•°æ®é›†ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def create_synthetic_dataset(self, dataset_name: str, local_path: Path) -> bool:
        """åˆ›å»ºåˆæˆæ•°æ®é›†ç”¨äºæ¼”ç¤º"""
        print(f"ğŸ¨ åˆ›å»ºåˆæˆæ•°æ®é›†: {dataset_name}")
        
        try:
            if dataset_name == "blind_road_basic":
                self.create_blind_road_basic_dataset(local_path)
            elif dataset_name == "unitree_blind_road":
                self.create_unitree_dataset(local_path)
            elif dataset_name == "blind_road_damage":
                self.create_damage_dataset(local_path)
            
            print(f"âœ… åˆæˆæ•°æ®é›†åˆ›å»ºå®Œæˆ: {local_path}")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºåˆæˆæ•°æ®é›†å¤±è´¥: {e}")
            return False
    
    def create_blind_road_basic_dataset(self, local_path: Path):
        """åˆ›å»ºåŸºç¡€ç›²é“æ•°æ®é›†"""
        # åˆ›å»ºç±»åˆ«ç›®å½•
        positive_dir = local_path / "positive"  # æœ‰ç›²é“
        negative_dir = local_path / "negative"  # æ— ç›²é“
        positive_dir.mkdir(exist_ok=True)
        negative_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆ233å¼ æœ‰ç›²é“çš„å›¾åƒ
        for i in tqdm(range(233), desc="ç”Ÿæˆæœ‰ç›²é“å›¾åƒ"):
            img = self.generate_blind_road_image(has_blind_road=True)
            cv2.imwrite(str(positive_dir / f"positive_{i:03d}.jpg"), img)
        
        # ç”Ÿæˆ148å¼ æ— ç›²é“çš„å›¾åƒ
        for i in tqdm(range(148), desc="ç”Ÿæˆæ— ç›²é“å›¾åƒ"):
            img = self.generate_blind_road_image(has_blind_road=False)
            cv2.imwrite(str(negative_dir / f"negative_{i:03d}.jpg"), img)
    
    def create_unitree_dataset(self, local_path: Path):
        """åˆ›å»ºUnitreeé£æ ¼æ•°æ®é›†"""
        images_dir = local_path / "images"
        masks_dir = local_path / "masks"
        images_dir.mkdir(exist_ok=True)
        masks_dir.mkdir(exist_ok=True)
        
        for i in tqdm(range(500), desc="ç”ŸæˆUnitreeæ•°æ®é›†"):
            # ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒ
            img = self.generate_high_res_blind_road_image()
            mask = self.generate_blind_road_mask(img.shape[:2])
            
            cv2.imwrite(str(images_dir / f"image_{i:04d}.jpg"), img)
            cv2.imwrite(str(masks_dir / f"mask_{i:04d}.png"), mask)
    
    def create_damage_dataset(self, local_path: Path):
        """åˆ›å»ºæŸåæ£€æµ‹æ•°æ®é›†"""
        images_dir = local_path / "images"
        annotations_dir = local_path / "annotations"
        images_dir.mkdir(exist_ok=True)
        annotations_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºYOLOæ ¼å¼æ ‡æ³¨
        yolo_annotations = []
        
        for i in tqdm(range(1000), desc="ç”ŸæˆæŸåæ£€æµ‹æ•°æ®é›†"):
            img, annotations = self.generate_damage_detection_image()
            
            # ä¿å­˜å›¾åƒ
            img_path = images_dir / f"damage_{i:04d}.jpg"
            cv2.imwrite(str(img_path), img)
            
            # ä¿å­˜YOLOæ ¼å¼æ ‡æ³¨
            yolo_path = annotations_dir / f"damage_{i:04d}.txt"
            with open(yolo_path, 'w') as f:
                for ann in annotations:
                    f.write(f"{ann['class_id']} {ann['x_center']:.6f} {ann['y_center']:.6f} "
                           f"{ann['width']:.6f} {ann['height']:.6f}\n")
            
            yolo_annotations.append({
                'image_path': str(img_path),
                'annotation_path': str(yolo_path),
                'annotations': annotations
            })
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'name': 'ç›²é“æŸåæ£€æµ‹æ•°æ®é›†',
            'description': 'åŒ…å«å„ç§ç›²é“æŸåæƒ…å†µçš„æ£€æµ‹æ•°æ®é›†',
            'classes': ['normal', 'damaged', 'obstacle'],
            'class_names': ['æ­£å¸¸ç›²é“', 'æŸåç›²é“', 'éšœç¢ç‰©'],
            'total_images': 1000,
            'annotations': yolo_annotations
        }
        
        with open(local_path / 'dataset_info.json', 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, ensure_ascii=False, indent=2)
    
    def generate_blind_road_image(self, has_blind_road: bool = True, size: tuple = (50, 50)) -> np.ndarray:
        """ç”Ÿæˆç›²é“å›¾åƒ"""
        img = np.random.randint(0, 255, (*size, 3), dtype=np.uint8)
        
        if has_blind_road:
            # æ·»åŠ ç›²é“çº¹ç†
            for i in range(0, size[0], 5):
                for j in range(0, size[1], 5):
                    if (i + j) % 10 == 0:
                        cv2.rectangle(img, (j, i), (j+3, i+3), (100, 100, 100), -1)
        
        return img
    
    def generate_high_res_blind_road_image(self, size: tuple = (640, 480)) -> np.ndarray:
        """ç”Ÿæˆé«˜åˆ†è¾¨ç‡ç›²é“å›¾åƒ"""
        img = np.random.randint(50, 200, (*size, 3), dtype=np.uint8)
        
        # æ·»åŠ é“è·¯çº¹ç†
        for i in range(0, size[0], 20):
            cv2.line(img, (0, i), (size[1], i), (120, 120, 120), 2)
        
        # æ·»åŠ ç›²é“çº¹ç†
        blind_road_width = 60
        start_x = (size[1] - blind_road_width) // 2
        
        for i in range(0, size[0], 10):
            for j in range(start_x, start_x + blind_road_width, 8):
                if (i + j) % 16 == 0:
                    cv2.rectangle(img, (j, i), (j+4, i+4), (80, 80, 80), -1)
        
        return img
    
    def generate_blind_road_mask(self, size: tuple) -> np.ndarray:
        """ç”Ÿæˆç›²é“åˆ†å‰²æ©ç """
        mask = np.zeros(size, dtype=np.uint8)
        
        # åœ¨å›¾åƒä¸­å¤®åˆ›å»ºç›²é“åŒºåŸŸ
        blind_road_width = 60
        start_x = (size[1] - blind_road_width) // 2
        
        mask[:, start_x:start_x + blind_road_width] = 255
        
        return mask
    
    def generate_damage_detection_image(self, size: tuple = (640, 480)) -> tuple:
        """ç”ŸæˆæŸåæ£€æµ‹å›¾åƒå’Œæ ‡æ³¨"""
        img = self.generate_high_res_blind_road_image(size)
        annotations = []
        
        # éšæœºæ·»åŠ æŸååŒºåŸŸ
        if np.random.random() > 0.3:  # 70%æ¦‚ç‡æœ‰æŸå
            # æŸååŒºåŸŸ
            x1 = np.random.randint(100, size[1]-200)
            y1 = np.random.randint(100, size[0]-200)
            x2 = x1 + np.random.randint(50, 150)
            y2 = y1 + np.random.randint(50, 150)
            
            # ç»˜åˆ¶æŸååŒºåŸŸ
            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), -1)
            
            # è½¬æ¢ä¸ºYOLOæ ¼å¼
            x_center = (x1 + x2) / 2 / size[1]
            y_center = (y1 + y2) / 2 / size[0]
            width = (x2 - x1) / size[1]
            height = (y2 - y1) / size[0]
            
            annotations.append({
                'class_id': 1,  # æŸåç›²é“
                'x_center': x_center,
                'y_center': y_center,
                'width': width,
                'height': height
            })
        
        # éšæœºæ·»åŠ éšœç¢ç‰©
        if np.random.random() > 0.5:  # 50%æ¦‚ç‡æœ‰éšœç¢ç‰©
            x1 = np.random.randint(50, size[1]-100)
            y1 = np.random.randint(50, size[0]-100)
            x2 = x1 + np.random.randint(30, 80)
            y2 = y1 + np.random.randint(30, 80)
            
            # ç»˜åˆ¶éšœç¢ç‰©
            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), -1)
            
            # è½¬æ¢ä¸ºYOLOæ ¼å¼
            x_center = (x1 + x2) / 2 / size[1]
            y_center = (y1 + y2) / 2 / size[0]
            width = (x2 - x1) / size[1]
            height = (y2 - y1) / size[0]
            
            annotations.append({
                'class_id': 2,  # éšœç¢ç‰©
                'x_center': x_center,
                'y_center': y_center,
                'width': width,
                'height': height
            })
        
        return img, annotations
    
    def convert_to_yolo_format(self, dataset_name: str) -> bool:
        """è½¬æ¢ä¸ºYOLOæ ¼å¼"""
        print(f"ğŸ”„ è½¬æ¢æ•°æ®é›†ä¸ºYOLOæ ¼å¼: {dataset_name}")
        
        try:
            raw_path = self.base_dir / "raw" / self.datasets[dataset_name]["local_path"]
            yolo_path = self.base_dir / "yolo_format" / dataset_name
            
            if not raw_path.exists():
                print(f"âŒ åŸå§‹æ•°æ®é›†ä¸å­˜åœ¨: {raw_path}")
                return False
            
            # åˆ›å»ºYOLOæ ¼å¼ç›®å½•
            yolo_path.mkdir(parents=True, exist_ok=True)
            (yolo_path / "images").mkdir(exist_ok=True)
            (yolo_path / "labels").mkdir(exist_ok=True)
            
            if dataset_name == "blind_road_damage":
                return self.convert_damage_dataset_to_yolo(raw_path, yolo_path)
            else:
                return self.convert_general_dataset_to_yolo(raw_path, yolo_path)
                
        except Exception as e:
            print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
            return False
    
    def convert_damage_dataset_to_yolo(self, raw_path: Path, yolo_path: Path) -> bool:
        """è½¬æ¢æŸåæ£€æµ‹æ•°æ®é›†ä¸ºYOLOæ ¼å¼"""
        images_dir = raw_path / "images"
        annotations_dir = raw_path / "annotations"
        
        if not images_dir.exists() or not annotations_dir.exists():
            print("âŒ å›¾åƒæˆ–æ ‡æ³¨ç›®å½•ä¸å­˜åœ¨")
            return False
        
        # å¤åˆ¶å›¾åƒæ–‡ä»¶
        for img_file in images_dir.glob("*.jpg"):
            shutil.copy2(img_file, yolo_path / "images" / img_file.name)
        
        # å¤åˆ¶æ ‡æ³¨æ–‡ä»¶
        for ann_file in annotations_dir.glob("*.txt"):
            shutil.copy2(ann_file, yolo_path / "labels" / ann_file.name)
        
        # åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶
        dataset_yaml = {
            'path': str(yolo_path.absolute()),
            'train': 'images',
            'val': 'images',
            'test': 'images',
            'nc': 3,
            'names': ['æ­£å¸¸ç›²é“', 'æŸåç›²é“', 'éšœç¢ç‰©']
        }
        
        with open(yolo_path / "dataset.yaml", 'w', encoding='utf-8') as f:
            yaml.dump(dataset_yaml, f, default_flow_style=False, allow_unicode=True)
        
        print(f"âœ… YOLOæ ¼å¼è½¬æ¢å®Œæˆ: {yolo_path}")
        return True
    
    def convert_general_dataset_to_yolo(self, raw_path: Path, yolo_path: Path) -> bool:
        """è½¬æ¢é€šç”¨æ•°æ®é›†ä¸ºYOLOæ ¼å¼"""
        # è¿™é‡Œå¯ä»¥å®ç°å…¶ä»–æ•°æ®é›†çš„è½¬æ¢é€»è¾‘
        print("âš ï¸ é€šç”¨æ•°æ®é›†è½¬æ¢åŠŸèƒ½å¾…å®ç°")
        return True
    
    def split_dataset(self, dataset_name: str, train_ratio: float = 0.7, 
                     val_ratio: float = 0.2, test_ratio: float = 0.1) -> bool:
        """åˆ†å‰²æ•°æ®é›†"""
        print(f"ğŸ“Š åˆ†å‰²æ•°æ®é›†: {dataset_name}")
        
        try:
            yolo_path = self.base_dir / "yolo_format" / dataset_name
            if not yolo_path.exists():
                print(f"âŒ YOLOæ ¼å¼æ•°æ®é›†ä¸å­˜åœ¨: {yolo_path}")
                return False
            
            # åˆ›å»ºåˆ†å‰²ç›®å½•
            for split in ['train', 'val', 'test']:
                (yolo_path / split / "images").mkdir(parents=True, exist_ok=True)
                (yolo_path / split / "labels").mkdir(parents=True, exist_ok=True)
            
            # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
            image_files = list((yolo_path / "images").glob("*.jpg"))
            np.random.shuffle(image_files)
            
            total_images = len(image_files)
            train_end = int(total_images * train_ratio)
            val_end = train_end + int(total_images * val_ratio)
            
            # åˆ†å‰²æ•°æ®é›†
            train_files = image_files[:train_end]
            val_files = image_files[train_end:val_end]
            test_files = image_files[val_end:]
            
            # å¤åˆ¶æ–‡ä»¶åˆ°å¯¹åº”ç›®å½•
            for files, split in [(train_files, 'train'), (val_files, 'val'), (test_files, 'test')]:
                for img_file in files:
                    # å¤åˆ¶å›¾åƒ
                    shutil.copy2(img_file, yolo_path / split / "images" / img_file.name)
                    
                    # å¤åˆ¶å¯¹åº”çš„æ ‡æ³¨æ–‡ä»¶
                    label_file = yolo_path / "labels" / img_file.with_suffix('.txt').name
                    if label_file.exists():
                        shutil.copy2(label_file, yolo_path / split / "labels" / label_file.name)
            
            print(f"âœ… æ•°æ®é›†åˆ†å‰²å®Œæˆ:")
            print(f"   è®­ç»ƒé›†: {len(train_files)} å¼ ")
            print(f"   éªŒè¯é›†: {len(val_files)} å¼ ")
            print(f"   æµ‹è¯•é›†: {len(test_files)} å¼ ")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®é›†åˆ†å‰²å¤±è´¥: {e}")
            return False
    
    def get_dataset_info(self, dataset_name: str) -> Dict:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        if dataset_name not in self.datasets:
            return {}
        
        info = self.datasets[dataset_name].copy()
        local_path = self.base_dir / "raw" / info["local_path"]
        
        if local_path.exists():
            info["local_exists"] = True
            info["file_count"] = len(list(local_path.rglob("*.*")))
        else:
            info["local_exists"] = False
            info["file_count"] = 0
        
        return info
    
    def list_available_datasets(self):
        """åˆ—å‡ºå¯ç”¨æ•°æ®é›†"""
        print("ğŸ“‹ å¯ç”¨æ•°æ®é›†åˆ—è¡¨:")
        print("=" * 60)
        
        for name, info in self.datasets.items():
            status = "âœ… å·²ä¸‹è½½" if (self.base_dir / "raw" / info["local_path"]).exists() else "âŒ æœªä¸‹è½½"
            print(f"åç§°: {info['name']}")
            print(f"æè¿°: {info['description']}")
            print(f"æ ¼å¼: {info['format']}")
            print(f"çŠ¶æ€: {status}")
            print("-" * 60)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ç›²é“æ£€æµ‹æ•°æ®é›†ä¸‹è½½å™¨")
    print("=" * 50)
    
    downloader = BlindRoadDatasetDownloader()
    
    # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
    downloader.list_available_datasets()
    
    # ä¸‹è½½æŸåæ£€æµ‹æ•°æ®é›†
    print("\nğŸ“¥ å¼€å§‹ä¸‹è½½æŸåæ£€æµ‹æ•°æ®é›†...")
    success = downloader.download_dataset("blind_road_damage")
    
    if success:
        # è½¬æ¢ä¸ºYOLOæ ¼å¼
        print("\nğŸ”„ è½¬æ¢ä¸ºYOLOæ ¼å¼...")
        downloader.convert_to_yolo_format("blind_road_damage")
        
        # åˆ†å‰²æ•°æ®é›†
        print("\nğŸ“Š åˆ†å‰²æ•°æ®é›†...")
        downloader.split_dataset("blind_road_damage")
        
        print("\nâœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
        print(f"æ•°æ®é›†ä½ç½®: {downloader.base_dir}")
    else:
        print("\nâŒ æ•°æ®é›†ä¸‹è½½å¤±è´¥")

if __name__ == "__main__":
    main()










